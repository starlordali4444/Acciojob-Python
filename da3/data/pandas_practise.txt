1. Missing Values: How many products are missing the `Item_Weight` information?
2. Fat Content Standardization: There are inconsistencies in the `Item_Fat_Content` labeling. How can you standardize the labels to have consistent naming?
3. Average Sales: What is the average sales amount for each category of `Item_Fat_Content`?
4. Total Sales per Year: What is the total sales generated by each `Outlet_Establishment_Year`?
5. Price Range: How many products fall under the high-price range where `Item_MRP` is above the 75th percentile?
6. Sales Efficiency: Which outlet has the highest sales efficiency, defined as sales per unit of `Item_Visibility`?
7. Product Popularity: Which product type is the most popular, based on the average sales per product?
8. Oldest Outlet Performance: How does the oldest outlet's performance compare to the newest in terms of average sales?
9. High Visibility Products: Do high visibility items (`Item_Visibility` > 0.05) result in higher sales?
10. Low Sales Inquiry: Which items have not achieved minimum expected sales (`Item_Outlet_Sales` < 100)?
11. Seasonal Sales: If you have monthly sales data, can you determine in which month the sales peak for 'Soft Drinks'?
12. Healthy vs. Unhealthy: Is there a significant difference in sales between low-fat and regular products?
13. Outlier Detection: Identify the items that have `Item_Outlet_Sales` far from the average of their respective `Item_Type`.
14. Stock Turnover: Calculate the stock turnover rate by comparing the number of orders to the quantity of goods sold.
15. Location Impact: Analyze the impact of `Outlet_Location_Type` on `Item_Outlet_Sales` to determine if certain locations perform better than others.


import pandas as pd

# Load your dataset (assuming it's a CSV file)
df = pd.read_csv('path_to_your_dataset.csv')


Now, let's solve each problem one by one:

1. Missing Values:

missing_weights = df['Item_Weight'].isnull().sum()


2. Fat Content Standardization:

df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'})


3. Average Sales:

average_sales_per_fat_content = df.groupby('Item_Fat_Content')['Item_Outlet_Sales'].mean()


4. Total Sales per Year:

total_sales_per_year = df.groupby('Outlet_Establishment_Year')['Item_Outlet_Sales'].sum()


5. Price Range:

high_price_range = df[df['Item_MRP'] > df['Item_MRP'].quantile(0.75)].shape[0]


6. Sales Efficiency:

sales_efficiency = df.assign(SalesEfficiency=df['Item_Outlet_Sales'] / df['Item_Visibility']).nlargest(1, 'SalesEfficiency')


7. Product Popularity:

popularity_by_type = df.groupby('Item_Type')['Item_Outlet_Sales'].mean().idxmax()


8. Oldest Outlet Performance:

oldest_outlet = df['Outlet_Establishment_Year'].min()
newest_outlet = df['Outlet_Establishment_Year'].max()
oldest_outlet_performance = df[df['Outlet_Establishment_Year'] == oldest_outlet]['Item_Outlet_Sales'].mean()
newest_outlet_performance = df[df['Outlet_Establishment_Year'] == newest_outlet]['Item_Outlet_Sales'].mean()


9. High Visibility Products:

high_visibility_sales = df[df['Item_Visibility'] > 0.05]['Item_Outlet_Sales'].mean()


10. Low Sales Inquiry:

low_sales_items = df[df['Item_Outlet_Sales'] < 100]['Item_Identifier']


11. Seasonal Sales:
Assuming you have a column `Sale_Month` extracted from `datetime`.

# df['Sale_Month'] = pd.to_datetime(df['datetime']).dt.month # Uncomment if you need to create the Sale_Month column
peak_sales_month = df[df['Item_Type'] == 'Soft Drinks'].groupby('Sale_Month')['Item_Outlet_Sales'].mean().idxmax()


12. Healthy vs. Unhealthy:

average_sales_low_fat = df[df['Item_Fat_Content'] == 'Low Fat']['Item_Outlet_Sales'].mean()
average_sales_regular = df[df['Item_Fat_Content'] == 'Regular']['Item_Outlet_Sales'].mean()


13. Outlier Detection:

mean_sales = df['Item_Outlet_Sales'].mean()
std_sales = df['Item_Outlet_Sales'].std()
outliers = df[(df['Item_Outlet_Sales'] < mean_sales - 3 * std_sales) | (df['Item_Outlet_Sales'] > mean_sales + 3 * std_sales)]


14. Stock Turnover:

# Assuming 'Item_Quantity' is a column representing the quantity of goods sold
# df['Stock_Turnover'] = df['Orders'] / df['Item_Quantity'] # Uncomment if you have Item_Quantity


15. Location Impact:

location_impact = df.groupby('Outlet_Location_Type')['Item_Outlet_Sales'].mean()



1. Discounted Products: How many products have an MRP higher than the average but have sales below the average sales, indicating possible discounts?

2. Weighty Sales: Is there a correlation between the weight of an item and its sales? Find out if heavier items tend to sell more or less.

3. Fat Content Sales Impact: Do 'Low Fat' products generate more sales than 'Regular' products within the 'Snack Foods' category?

4. Revenue by Year: What is the total revenue generated each year? Assume revenue is a direct function of `Item_Outlet_Sales`.

5. Product Variety: Which `Outlet_Identifier` offers the widest variety of `Item_Type`?

6. Average Item Visibility: What is the average visibility of items in the 'Household' category compared to the overall average?

7. Pricing Strategy: What is the standard deviation of the MRP (`Item_MRP`) of 'Dairy' products? Does this suggest a varied pricing strategy?

8. Healthy Options: How many unique 'Low Fat' products are there compared to 'Regular' products?

9. Store Comparison: Compare the mean sales of the smallest and largest `Outlet_Size` stores. Which one performs better?

10. Age of Inventory: Calculate the age of the inventory by subtracting the `Outlet_Establishment_Year` from the current year, then find out which age group has the highest average sales.

11. Peak Sales Items: Which item had the highest sales in the most recent year in the dataset?

12. Product Replacement: Identify products that have zero visibility but have sales recorded, suggesting a possible error in data recording.

13. Category Sales Contribution: What percentage of total sales does the 'Fruits and Vegetables' category contribute to?

14. Sales and Establishment Year: Is there a trend in sales with respect to the `Outlet_Establishment_Year`? Do older outlets have higher sales?

15. Location-Based Sales: Within each `Outlet_Location_Type`, which `Outlet_Type` has the highest average sales, and how does this compare to the average sales of all outlets?



current_year = 2021  # Replace with the actual current year

# 1. Discounted Products
average_mrp = df['Item_MRP'].mean()
average_sales = df['Item_Outlet_Sales'].mean()
discounted_products = df[(df['Item_MRP'] > average_mrp) & (df['Item_Outlet_Sales'] < average_sales)]

# 2. Weighty Sales
correlation_weight_sales = df['Item_Weight'].corr(df['Item_Outlet_Sales'])

# 3. Fat Content Sales Impact
low_fat_snack_sales = df[(df['Item_Fat_Content'] == 'Low Fat') & (df['Item_Type'] == 'Snack Foods')]['Item_Outlet_Sales'].sum()
regular_snack_sales = df[(df['Item_Fat_Content'] == 'Regular') & (df['Item_Type'] == 'Snack Foods')]['Item_Outlet_Sales'].sum()

# 4. Revenue by Year
revenue_by_year = df.groupby('Outlet_Establishment_Year')['Item_Outlet_Sales'].sum()

# 5. Product Variety
product_variety = df.groupby('Outlet_Identifier')['Item_Type'].nunique()

# 6. Average Item Visibility
average_visibility_household = df[df['Item_Type'] == 'Household']['Item_Visibility'].mean()
overall_average_visibility = df['Item_Visibility'].mean()

# 7. Pricing Strategy
std_dev_dairy_mrp = df[df['Item_Type'] == 'Dairy']['Item_MRP'].std()

# 8. Healthy Options
unique_low_fat = df[df['Item_Fat_Content'] == 'Low Fat']['Item_Identifier'].nunique()
unique_regular = df[df['Item_Fat_Content'] == 'Regular']['Item_Identifier'].nunique()

# 9. Store Comparison
mean_sales_small = df[df['Outlet_Size'] == 'Small']['Item_Outlet_Sales'].mean()
mean_sales_large = df[df['Outlet_Size'] == 'High']['Item_Outlet_Sales'].mean()

# 10. Age of Inventory
df['Inventory_Age'] = current_year - df['Outlet_Establishment_Year']
age_group_sales = df.groupby('Inventory_Age')['Item_Outlet_Sales'].mean()

# 11. Peak Sales Items
most_recent_year = df['Outlet_Establishment_Year'].max()
peak_sales_item = df[df['Outlet_Establishment_Year'] == most_recent_year]['Item_Outlet_Sales'].idxmax()

# 12. Product Replacement
zero_visibility_sales = df[(df['Item_Visibility'] == 0) & (df['Item_Outlet_Sales'] > 0)]

# 13. Category Sales Contribution
total_sales = df['Item_Outlet_Sales'].sum()
fruits_veggies_sales = df[df['Item_Type'] == 'Fruits and Vegetables']['Item_Outlet_Sales'].sum()
fruits_veggies_percentage = (fruits_veggies_sales / total_sales) * 100

# 14. Sales and Establishment Year
sales_trend_by_year = df.groupby('Outlet_Establishment_Year')['Item_Outlet_Sales'].mean()

# 15. Location-Based Sales
location_based_sales = df.groupby(['Outlet_Location_Type', 'Outlet_Type'])['Item_Outlet_Sales'].mean().reset_index()



1. Calculate the total number of videos posted by all the accounts in the dataset.
2. Find the account with the longest biography. What is the length of that biography?
3. Identify the top 5 accounts with the highest average engagement rate.
4. Determine the ratio of verified to non-verified accounts in the dataset.
5. List the account IDs of those who have more following than followers.
6. What is the average number of likes across all accounts?
7. Find the account with the most recent 'create_time' and display its nickname.
8. Calculate the median number of followers for all accounts.
9. Identify which day of the week had the most account creation activity.
10. Count how many accounts have no bio link provided.
11. Create a new column 'likes_per_video' by dividing 'likes' by 'videos_count'.
12. Find the account with the maximum 'like_engagement_rate' and display its biography.
13. Sort the accounts by the 'followers' count in descending order.
14. Determine the average number of videos posted by accounts with more than 1000 followers.
15. Extract and list all unique top video URLs from the dataset.


import pandas as pd

# Replace 'your_dataset.csv' with the path to your CSV file
df = pd.read_csv('your_dataset.csv')


Now, let's address each word problem:

1. Total number of videos posted by all accounts:
   
   total_videos = df['videos_count'].sum()
   

2. Account with the longest biography:
   
   df['bio_length'] = df['biography'].str.len()
   longest_bio_account = df[df['bio_length'] == df['bio_length'].max()]['account_id']
   

3. Top 5 accounts with the highest average engagement rate:
   
   top5_engagement = df.nlargest(5, 'awg_engagement_rate')['account_id']
   

4. Ratio of verified to non-verified accounts:
   
   verified_ratio = df['is_verified'].value_counts(normalize=True)
   

5. Accounts with more following than followers:
   
   more_following = df[df['following'] > df['followers']]['account_id']
   

6. Average number of likes across all accounts:
   
   avg_likes = df['likes'].mean()
   

7. Account with the most recent 'create_time':
   
   # Assuming 'create_time' is in a proper datetime format
   df['create_time'] = pd.to_datetime(df['create_time'])
   most_recent_account = df[df['create_time'] == df['create_time'].max()]['nickname']
   

8. Median number of followers:
   
   median_followers = df['followers'].median()
   

9. Day of the week with the most account creation activity:
   
   df['create_day'] = df['create_time'].dt.day_name()
   most_active_day = df['create_day'].value_counts().idxmax()
   

10. Count of accounts with no bio link:
    
    no_bio_link_count = df[df['bio_link'].isnull()].shape[0]
    

11. New column 'likes_per_video':
    
    df['likes_per_video'] = df['likes'] / df['videos_count']
    

12. Account with the highest 'like_engagement_rate':
    
    highest_like_engagement = df[df['like_engagement_rate'] == df['like_engagement_rate'].max()]['biography']
    

13. Accounts sorted by 'followers' in descending order:
    
    sorted_accounts = df.sort_values(by='followers', ascending=False)
    

14. Average number of videos posted by accounts with more than 1000 followers:
    
    avg_videos_1000_followers = df[df['followers'] > 1000]['videos_count'].mean()
    

15. Unique top video URLs:
    
    unique_videos = df['top_videos'].str.split(',').explode().unique()
    

1. Find the account with the shortest biography.
2. Calculate the total number of likes received by verified accounts.
3. Determine the average number of followers for accounts with no bio link.
4. List the account IDs of the top 10 accounts by 'comment_engagement_rate'.
5. Identify the account with the lowest 'awg_engagement_rate' but more than 500 followers.
6. Count the number of accounts created on the first day of a month.
7. Find the total number of accounts that have more likes than followers.
8. Calculate the standard deviation of the 'like_engagement_rate' across all accounts.
9. Determine the average number of videos posted by non-verified accounts.
10. List the nicknames of all accounts that have exactly 100 videos.
11. Find the account with the highest ratio of 'followers' to 'following'.
12. Count how many accounts have their 'create_time' on a weekend.
13. Identify the nickname of the account with the longest URL in 'profile_pic_url'.
14. Calculate the percentage of accounts that have a 'bio_link'.
15. Determine the median 'comment_engagement_rate' for accounts with more than 1000 likes.

1. Account with the shortest biography:
   
   shortest_bio_account = df[df['biography'].str.len() == df['biography'].str.len().min()]['account_id']
   

2. Total likes received by verified accounts:
   
   total_likes_verified = df[df['is_verified'] == True]['likes'].sum()
   

3. Average followers for accounts with no bio link:
   
   avg_followers_no_bio = df[df['bio_link'].isnull()]['followers'].mean()
   

4. Top 10 accounts by 'comment_engagement_rate':
   
   top10_comment_engagement = df.nlargest(10, 'comment_engagement_rate')['account_id']
   

5. Account with lowest 'awg_engagement_rate' but more than 500 followers:
   
   low_engagement_high_followers = df[(df['followers'] > 500) & (df['awg_engagement_rate'] == df[df['followers'] > 500]['awg_engagement_rate'].min())]['account_id']
   

6. Accounts created on the first day of a month:
   
   accounts_first_day = df[pd.to_datetime(df['create_time']).dt.day == 1].shape[0]
   

7. Accounts with more likes than followers:
   
   more_likes_than_followers = df[df['likes'] > df['followers']].shape[0]
   

8. Standard deviation of 'like_engagement_rate':
   
   std_like_engagement = df['like_engagement_rate'].std()
   

9. Average videos posted by non-verified accounts:
    
    avg_videos_non_verified = df[df['is_verified'] == False]['videos_count'].mean()
    

10. Nicknames of accounts with exactly 100 videos:
    
    nicknames_100_videos = df[df['videos_count'] == 100]['nickname']
    

11. Account with the highest ratio of 'followers' to 'following':
    
    highest_followers_following_ratio = df[df['following'] != 0]  # To avoid division by zero
    highest_followers_following_ratio = highest_followers_following_ratio[highest_followers_following_ratio['followers'] / highest_followers_following_ratio['following'] == (highest_followers_following_ratio['followers'] / highest_followers_following_ratio['following']).max()]['account_id']
    

12. Accounts with 'create_time' on a weekend:
    
    weekend_accounts = df[pd.to_datetime(df['create_time']).dt.dayofweek >= 5].shape[0]
    

13. Nickname with the longest URL in 'profile_pic_url':
    
    longest_url_nickname = df[df['profile_pic_url'].str.len() == df['profile_pic_url'].str.len().max()]['nickname']
    

14. Percentage of accounts with a 'bio_link':
    
    percentage_with_bio_link = (df['bio_link'].notnull().sum() / df.shape[0]) * 100
    

15. Median 'comment_engagement_rate' for accounts with more than 1000 likes:
    
    median_comment_engagement_1000_likes = df[df['likes'] > 1000]['comment_engagement_rate'].median()
    

Each solution corresponds to the respective word problem. Adjust the column names and formats according to your actual dataset. Remember to handle any missing or malformed data appropriately for accurate results.